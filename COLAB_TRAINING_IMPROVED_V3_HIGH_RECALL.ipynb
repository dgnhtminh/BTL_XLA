{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140e442e",
   "metadata": {},
   "source": [
    "## 0. CÃ i Ä‘áº·t vÃ  Setup mÃ´i trÆ°á»ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gá»¡ bá» phiÃªn báº£n hiá»‡n táº¡i Ä‘á»ƒ trÃ¡nh lá»—i\n",
    "!pip uninstall -y numpy opencv-python opencv-python-headless stardist csbdeep\n",
    "\n",
    "# CÃ i Ä‘áº·t phiÃªn báº£n tÆ°Æ¡ng thÃ­ch (NumPy 1.x cho Colab)\n",
    "!pip install \"numpy<2.0.0\" \"opencv-python-headless<4.10\"\n",
    "!pip install stardist csbdeep\n",
    "\n",
    "print(\"âœ… ÄÃ£ cÃ i Ä‘áº·t xong. VUI LÃ’NG KHá»I Äá»˜NG Láº I RUNTIME!\")\n",
    "print(\"   Runtime > Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch cho Python 3.12 compatibility\n",
    "import configparser\n",
    "\n",
    "if not hasattr(configparser, 'SafeConfigParser'):\n",
    "    configparser.SafeConfigParser = configparser.ConfigParser\n",
    "    print(\"âœ… ÄÃ£ vÃ¡ lá»—i SafeConfigParser thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530ee80",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive vÃ  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Táº¡o thÆ° má»¥c lÃ m viá»‡c\n",
    "work_dir = '/content/stardist_project'\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)\n",
    "\n",
    "print(f\"âœ… Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdad31",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# ÄÆ°á»ng dáº«n file trÃªn Google Drive\n",
    "drive_dataset_path = '/content/drive/MyDrive/my_dataset.rar'\n",
    "local_filename = 'my_dataset.rar'\n",
    "\n",
    "if os.path.exists(drive_dataset_path):\n",
    "    print(\"Copying from Google Drive...\")\n",
    "    shutil.copy2(drive_dataset_path, local_filename)\n",
    "    \n",
    "    # CÃ i unrar vÃ  giáº£i nÃ©n\n",
    "    !apt-get install unrar -qq\n",
    "    print(\"Extracting RAR file...\")\n",
    "    \n",
    "    result = subprocess.run(['unrar', 'x', '-y', local_filename],\n",
    "                            capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Dataset copied and extracted successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Lá»—i khi giáº£i nÃ©n:\", result.stderr)\n",
    "else:\n",
    "    print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file táº¡i: {drive_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556db1b8",
   "metadata": {},
   "source": [
    "## 3. Import Libraries vÃ  Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from csbdeep.utils import Path, normalize\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents\n",
    "from stardist.models import Config2D, StarDist2D\n",
    "\n",
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"\\nâœ… GPU is available and ready!\")\n",
    "    !nvidia-smi --query-gpu=gpu_name,memory.total --format=csv\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No GPU found. Training sáº½ ráº¥t cháº­m!\")\n",
    "    print(\"   HÃ£y báº­t GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc54b3",
   "metadata": {},
   "source": [
    "## 4. Load vÃ  kiá»ƒm tra dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ac03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c\n",
    "!ls -la my_dataset/\n",
    "!echo \"\\n=== TRAIN ===\"\n",
    "!ls -la my_dataset/train/\n",
    "!echo \"\\n=== VAL ===\"\n",
    "!ls -la my_dataset/val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb02586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    \"\"\"Load images vÃ  masks tá»« thÆ° má»¥c\"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    # Load images\n",
    "    img_files = sorted((base_path / 'images').glob('*'))\n",
    "    X = [np.array(Image.open(f)) for f in tqdm(img_files, desc=f\"Loading {base_dir}\")]\n",
    "    \n",
    "    # Load masks\n",
    "    mask_files = sorted((base_path / 'masks').glob('*'))\n",
    "    Y = [np.array(Image.open(f)) for f in mask_files]\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Load training vÃ  validation data\n",
    "X_train, Y_train = load_dataset('my_dataset/train')\n",
    "X_val, Y_val = load_dataset('my_dataset/val')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Images: {X_train.shape}\")\n",
    "print(f\"  Masks: {Y_train.shape}\")\n",
    "print(f\"  Total cells: {sum([len(np.unique(mask))-1 for mask in Y_train])}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Images: {X_val.shape}\")\n",
    "print(f\"  Masks: {Y_val.shape}\")\n",
    "print(f\"  Total cells: {sum([len(np.unique(mask))-1 for mask in Y_val])}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fe8cc",
   "metadata": {},
   "source": [
    "## 5. Normalize dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d18e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "axis_norm = (0,1)   # normalize channels independently\n",
    "\n",
    "X_train_norm = [normalize(x, 1, 99.8, axis=axis_norm) for x in tqdm(X_train, desc=\"Normalizing train\")]\n",
    "X_val_norm = [normalize(x, 1, 99.8, axis=axis_norm) for x in tqdm(X_val, desc=\"Normalizing val\")]\n",
    "\n",
    "print(\"âœ… Normalization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c3636",
   "metadata": {},
   "source": [
    "## 6. Visualize má»™t sá»‘ máº«u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3863719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stardist import random_label_cmap\n",
    "\n",
    "lbl_cmap = random_label_cmap()\n",
    "\n",
    "# Hiá»ƒn thá»‹ 6 máº«u training\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, len(X_train))\n",
    "    \n",
    "    axes[i, 0].imshow(X_train[idx], cmap='gray' if X_train[idx].ndim == 2 else None)\n",
    "    axes[i, 0].set_title(f'Training Image {idx}')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(Y_train[idx], cmap=lbl_cmap)\n",
    "    axes[i, 1].set_title(f'Ground Truth ({len(np.unique(Y_train[idx]))-1} cells)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(X_train[idx], cmap='gray' if X_train[idx].ndim == 2 else None)\n",
    "    axes[i, 2].imshow(Y_train[idx], cmap=lbl_cmap, alpha=0.5)\n",
    "    axes[i, 2].set_title('Overlay')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(X_train_norm[idx], cmap='gray' if X_train[idx].ndim == 2 else None)\n",
    "    axes[i, 3].set_title('Normalized')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04741037",
   "metadata": {},
   "source": [
    "## 7. ğŸš€ Configuration V3 - OPTIMIZED FOR HIGH RECALL\n",
    "\n",
    "### Key Changes tá»« V2:\n",
    "1. **n_rays: 64 â†’ 96** (tÄƒng 50%) â†’ Detect hÃ¬nh dáº¡ng phá»©c táº¡p tá»‘t hÆ¡n\n",
    "2. **prob_thresh: 0.479 â†’ 0.3** â†’ Giáº£m ngÆ°á»¡ng = Ã­t bá» sÃ³t hÆ¡n\n",
    "3. **epochs: 100 â†’ 150** â†’ Train lÃ¢u hÆ¡n\n",
    "4. **patience: 10 â†’ 15** â†’ Chá» lÃ¢u hÆ¡n trÆ°á»›c khi early stop\n",
    "5. **Augmentation máº¡nh hÆ¡n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stardist.models import Config2D, StarDist2D\n",
    "\n",
    "# TÃ­nh n_channel tá»« dá»¯ liá»‡u\n",
    "n_channel = 1 if X_train[0].ndim == 2 else X_train[0].shape[-1]\n",
    "\n",
    "print(f\"Input channels: {n_channel}\")\n",
    "\n",
    "# Configuration V3 - HIGH RECALL\n",
    "conf = Config2D(\n",
    "    # Architecture - TÄ‚NG n_rays\n",
    "    n_rays=96,  # â¬†ï¸ 64 â†’ 96 (tÄƒng 50%)\n",
    "    grid=(2,2),\n",
    "    n_channel_in=n_channel,\n",
    "    \n",
    "    # Training parameters - TRAIN LÃ‚U HÆ N\n",
    "    train_epochs=150,  # â¬†ï¸ 100 â†’ 150\n",
    "    train_steps_per_epoch=200,  # â¬†ï¸ 150 â†’ 200\n",
    "    train_batch_size=4,\n",
    "    train_learning_rate=0.0003,\n",
    "    train_reduce_lr={'factor': 0.5, 'patience': 8, 'min_delta': 0},  # â¬†ï¸ patience 5â†’8\n",
    "    \n",
    "    # Loss weights - TÄ‚NG TRá»ŒNG Sá» DIST\n",
    "    train_dist_loss='mae',\n",
    "    train_loss_weights=(1.0, 0.3),  # â¬†ï¸ (1,0.2) â†’ (1,0.3)\n",
    "    \n",
    "    # Augmentation - Máº NH HÆ N\n",
    "    use_gpu=True,\n",
    "    train_tensorboard=True,\n",
    ")\n",
    "\n",
    "print(conf)\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609c7c5",
   "metadata": {},
   "source": [
    "## 8. Táº¡o model vá»›i tÃªn má»›i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'stardist_v3_high_recall'\n",
    "model_basedir = 'models'\n",
    "\n",
    "model = StarDist2D(conf, name=model_name, basedir=model_basedir)\n",
    "\n",
    "print(f\"âœ… Model created: {model_name}\")\n",
    "print(f\"   Basedir: {model_basedir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673d0c4",
   "metadata": {},
   "source": [
    "## 9. ğŸ¨ Data Augmentation - ENHANCED\n",
    "\n",
    "### Augmentation máº¡nh hÆ¡n V2:\n",
    "- âœ… Rotation: 360Â° (toÃ n pháº§n)\n",
    "- âœ… Flip: horizontal + vertical\n",
    "- âœ… Elastic deformation: TÄ‚NG intensity\n",
    "- âœ… Brightness/Contrast: TÄ‚NG range\n",
    "- âœ… **Scale variation: 0.8-1.2x** (Má»šI)\n",
    "- âœ… **Gaussian noise** (Má»šI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13059707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stardist.models import StarDist2D\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Enhanced augmentation\n",
    "aug = iaa.Sequential([\n",
    "    # Geometric transforms\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.Rotate((-180, 180)),  # Full rotation\n",
    "    \n",
    "    # â­ THÃŠM Scale variation\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "        mode='reflect'\n",
    "    ),\n",
    "    \n",
    "    # Elastic deformation - TÄ‚NG alpha\n",
    "    iaa.Sometimes(0.5, iaa.ElasticTransformation(\n",
    "        alpha=(40, 80),  # â¬†ï¸ 30-60 â†’ 40-80\n",
    "        sigma=8\n",
    "    )),\n",
    "    \n",
    "    # Intensity transforms - TÄ‚NG range\n",
    "    iaa.Sometimes(0.5, iaa.Multiply((0.7, 1.4))),  # â¬†ï¸ 0.8-1.3 â†’ 0.7-1.4\n",
    "    iaa.Sometimes(0.5, iaa.Add((-20, 20))),  # â¬†ï¸ -15,15 â†’ -20,20\n",
    "    iaa.Sometimes(0.5, iaa.GammaContrast((0.7, 1.5))),  # â¬†ï¸ 0.8-1.3 â†’ 0.7-1.5\n",
    "    \n",
    "    # â­ THÃŠM Gaussian noise\n",
    "    iaa.Sometimes(0.3, iaa.AdditiveGaussianNoise(scale=(0, 0.05*255))),\n",
    "    \n",
    "    # Gaussian blur\n",
    "    iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 1.5))),\n",
    "])\n",
    "\n",
    "print(\"âœ… Enhanced augmentation pipeline created!\")\n",
    "print(\"\\nğŸ“Š Augmentation summary:\")\n",
    "print(\"   - Rotation: Full 360Â°\")\n",
    "print(\"   - Flip: Horizontal + Vertical\")\n",
    "print(\"   - Scale: 0.8x - 1.2x\")\n",
    "print(\"   - Elastic deform: alpha 40-80\")\n",
    "print(\"   - Brightness: Â±20\")\n",
    "print(\"   - Contrast: 0.7-1.5x\")\n",
    "print(\"   - Gaussian noise: 5% intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420e2ad",
   "metadata": {},
   "source": [
    "## 10. Test augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test augmentation trÃªn 1 sample\n",
    "test_idx = 0\n",
    "test_img = X_train_norm[test_idx]\n",
    "test_mask = Y_train[test_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Augment\n",
    "    aug_det = aug.to_deterministic()\n",
    "    img_aug = aug_det.augment_image(test_img)\n",
    "    mask_aug = aug_det.augment_image(test_mask)\n",
    "    \n",
    "    axes[0, i].imshow(img_aug, cmap='gray' if img_aug.ndim == 2 else None)\n",
    "    axes[0, i].set_title(f'Augmented Image {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(img_aug, cmap='gray' if img_aug.ndim == 2 else None)\n",
    "    axes[1, i].imshow(mask_aug, cmap=lbl_cmap, alpha=0.5)\n",
    "    axes[1, i].set_title(f'With Mask')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Augmentation test OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7fcf4",
   "metadata": {},
   "source": [
    "## 11. ğŸš‚ START TRAINING - V3 HIGH RECALL\n",
    "\n",
    "### Training vá»›i:\n",
    "- â° 150 epochs (thay vÃ¬ 100)\n",
    "- ğŸ¯ Early stopping patience = 15 (thay vÃ¬ 10)\n",
    "- ğŸ¨ Enhanced augmentation\n",
    "- ğŸ“Š TensorBoard logging\n",
    "\n",
    "**Dá»± kiáº¿n: ~45-60 phÃºt trÃªn T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09816aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"ğŸš‚ STARTING TRAINING V3 - HIGH RECALL\")\n",
    "print(f\"â° Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train vá»›i early stopping patience tÄƒng lÃªn\n",
    "history = model.train(\n",
    "    X_train_norm, Y_train,\n",
    "    validation_data=(X_val_norm, Y_val),\n",
    "    augmenter=aug,\n",
    "    epochs=150,  # â¬†ï¸ 100 â†’ 150\n",
    "    steps_per_epoch=200,  # â¬†ï¸ 150 â†’ 200\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… TRAINING COMPLETED!\")\n",
    "print(f\"â° End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"â±ï¸ Duration: {duration}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197361a",
   "metadata": {},
   "source": [
    "## 12. Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c18cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert history to DataFrame\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(hist_df['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(hist_df['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "if 'acc' in hist_df.columns:\n",
    "    axes[0, 1].plot(hist_df['acc'], label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(hist_df['val_acc'], label='Val Acc', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Training & Validation Accuracy', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "if 'lr' in hist_df.columns:\n",
    "    axes[1, 0].plot(hist_df['lr'], linewidth=2, color='orange')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Summary statistics\n",
    "axes[1, 1].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "TRAINING SUMMARY V3 - HIGH RECALL\n",
    "{'='*40}\n",
    "\n",
    "Configuration:\n",
    "  â€¢ n_rays: 96 (â†‘ from 64)\n",
    "  â€¢ Epochs trained: {len(hist_df)}\n",
    "  â€¢ Best val_loss: {hist_df['val_loss'].min():.4f}\n",
    "  â€¢ Final train loss: {hist_df['loss'].iloc[-1]:.4f}\n",
    "  â€¢ Final val loss: {hist_df['val_loss'].iloc[-1]:.4f}\n",
    "\n",
    "Improvements:\n",
    "  âœ“ Increased n_rays (64â†’96)\n",
    "  âœ“ Longer training (150 epochs)\n",
    "  âœ“ Enhanced augmentation\n",
    "  âœ“ Scale variation added\n",
    "  âœ“ Gaussian noise added\n",
    "\n",
    "Expected:\n",
    "  â€¢ Recall: 85-90% (â†‘ from 76.3%)\n",
    "  â€¢ Precision: 80-85%\n",
    "  â€¢ F1-Score: 82-87%\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "                verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_v3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training history saved to training_history_v3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a8ddd",
   "metadata": {},
   "source": [
    "## 13. ğŸ¯ Optimize thresholds vá»›i prob_thresh tháº¥p hÆ¡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stardist.matching import matching_dataset\n",
    "\n",
    "# GIáº¢M prob_thresh xuá»‘ng 0.3 Ä‘á»ƒ tÄƒng Recall\n",
    "model.thresholds.prob = 0.3  # â¬‡ï¸ 0.479 â†’ 0.3\n",
    "model.thresholds.nms = 0.3\n",
    "\n",
    "print(\"ğŸ¯ Testing with LOWER thresholds for HIGH RECALL:\")\n",
    "print(f\"   prob_thresh: {model.thresholds.prob}\")\n",
    "print(f\"   nms_thresh: {model.thresholds.nms}\")\n",
    "print(f\"\\nâ³ Optimizing thresholds on validation set...\\n\")\n",
    "\n",
    "# Optimize\n",
    "model.optimize_thresholds(\n",
    "    X_val_norm[:20],  # Use 20 samples\n",
    "    Y_val[:20],\n",
    "    nms_threshs=[0.2, 0.3, 0.4],  # Test lower NMS\n",
    "    iou_threshs=[0.5]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Optimized thresholds:\")\n",
    "print(f\"   prob_thresh: {model.thresholds.prob:.3f}\")\n",
    "print(f\"   nms_thresh: {model.thresholds.nms:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcaca7",
   "metadata": {},
   "source": [
    "## 14. ğŸ“Š Evaluate trÃªn validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict trÃªn validation set\n",
    "Y_val_pred = []\n",
    "\n",
    "print(\"Predicting on validation set...\")\n",
    "for img in tqdm(X_val_norm[:20]):  # Test 20 samples\n",
    "    labels, _ = model.predict_instances(img, n_tiles=model._guess_n_tiles(img))\n",
    "    Y_val_pred.append(labels)\n",
    "\n",
    "# Calculate metrics at IoU 0.5\n",
    "stats = matching_dataset(Y_val[:20], Y_val_pred, thresh=0.5, show_progress=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š VALIDATION METRICS (IoU 0.5)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nğŸ¯ Precision: {stats.precision:.1%}\")\n",
    "print(f\"ğŸ¯ Recall: {stats.recall:.1%}\")\n",
    "print(f\"ğŸ¯ F1-Score: {stats.f1:.1%}\")\n",
    "print(f\"ğŸ¯ AP: {stats.mean_matched_score:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON WITH V2\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"                 V2      â†’    V3 (Target)\")\n",
    "print(f\"Precision:     81.7%   â†’    80-85%\")\n",
    "print(f\"Recall:        76.3%   â†’    85-90% â­\")\n",
    "print(f\"F1-Score:      78.9%   â†’    82-87%\")\n",
    "\n",
    "if stats.recall >= 0.85:\n",
    "    print(f\"\\nâœ…âœ…âœ… RECALL TARGET ACHIEVED! ({stats.recall:.1%})\")\n",
    "    print(\"ğŸ‰ Model V3 Ä‘áº¡t má»¥c tiÃªu 85-90%!\")\n",
    "elif stats.recall >= 0.80:\n",
    "    print(f\"\\nâœ…âœ… RECALL IMPROVED! ({stats.recall:.1%})\")\n",
    "    print(\"ğŸ’¡ Gáº§n Ä‘áº¡t má»¥c tiÃªu, cÃ³ thá»ƒ giáº£m prob_thresh thÃªm\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ RECALL NEEDS MORE WORK ({stats.recall:.1%})\")\n",
    "    print(\"ğŸ’¡ Thá»­ train thÃªm hoáº·c Ä‘iá»u chá»‰nh prob_thresh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef781a97",
   "metadata": {},
   "source": [
    "## 15. Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca484e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 6 samples\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    idx = i\n",
    "    \n",
    "    # Original image\n",
    "    axes[i, 0].imshow(X_val[idx], cmap='gray' if X_val[idx].ndim == 2 else None)\n",
    "    axes[i, 0].set_title(f'Val Image {idx}')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(Y_val[idx], cmap=lbl_cmap)\n",
    "    n_true = len(np.unique(Y_val[idx])) - 1\n",
    "    axes[i, 1].set_title(f'Ground Truth ({n_true} cells)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(Y_val_pred[idx], cmap=lbl_cmap)\n",
    "    n_pred = len(np.unique(Y_val_pred[idx])) - 1\n",
    "    axes[i, 2].set_title(f'Prediction ({n_pred} cells)')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[i, 3].imshow(X_val[idx], cmap='gray' if X_val[idx].ndim == 2 else None)\n",
    "    axes[i, 3].imshow(Y_val_pred[idx], cmap=lbl_cmap, alpha=0.5)\n",
    "    axes[i, 3].set_title('Overlay')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('validation_results_v3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Validation results saved to validation_results_v3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e6485",
   "metadata": {},
   "source": [
    "## 16. ğŸ’¾ Save model vÃ  copy vá» Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Export model\n",
    "model.export_TF()\n",
    "\n",
    "# Save training summary\n",
    "summary_file = f\"{model_basedir}/{model_name}/training_summary_v3.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(f\"STARDIST V3 - HIGH RECALL TRAINING SUMMARY\\n\")\n",
    "    f.write(f\"={'='*60}\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {model_name}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Configuration:\\n\")\n",
    "    f.write(f\"  - n_rays: 96 (â†‘ from 64)\\n\")\n",
    "    f.write(f\"  - grid: (2,2)\\n\")\n",
    "    f.write(f\"  - epochs: {len(hist_df)}\\n\")\n",
    "    f.write(f\"  - prob_thresh: {model.thresholds.prob:.3f}\\n\")\n",
    "    f.write(f\"  - nms_thresh: {model.thresholds.nms:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Training Data:\\n\")\n",
    "    f.write(f\"  - Train samples: {len(X_train)}\\n\")\n",
    "    f.write(f\"  - Val samples: {len(X_val)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Final Metrics (IoU 0.5):\\n\")\n",
    "    f.write(f\"  - Precision: {stats.precision:.3f}\\n\")\n",
    "    f.write(f\"  - Recall: {stats.recall:.3f}\\n\")\n",
    "    f.write(f\"  - F1-Score: {stats.f1:.3f}\\n\")\n",
    "    f.write(f\"  - AP: {stats.mean_matched_score:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Improvements from V2:\\n\")\n",
    "    f.write(f\"  âœ“ Increased n_rays (64â†’96)\\n\")\n",
    "    f.write(f\"  âœ“ Decreased prob_thresh (0.479â†’{model.thresholds.prob:.3f})\\n\")\n",
    "    f.write(f\"  âœ“ Enhanced augmentation with scale + noise\\n\")\n",
    "    f.write(f\"  âœ“ Longer training (150 epochs)\\n\")\n",
    "    f.write(f\"  âœ“ Recall improved: 76.3% â†’ {stats.recall:.1%}\\n\")\n",
    "\n",
    "print(f\"âœ… Training summary saved to: {summary_file}\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "drive_save_path = f'/content/drive/MyDrive/stardist_models/{model_name}'\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "print(f\"\\nCopying model to Google Drive...\")\n",
    "shutil.copytree(f\"{model_basedir}/{model_name}\", \n",
    "                f\"{drive_save_path}\",\n",
    "                dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ… MODEL SAVED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Local: {model_basedir}/{model_name}\")\n",
    "print(f\"Drive: {drive_save_path}\")\n",
    "print(f\"\\nğŸ“Š Validation Metrics:\")\n",
    "print(f\"   Precision: {stats.precision:.1%}\")\n",
    "print(f\"   Recall: {stats.recall:.1%} {'âœ…' if stats.recall >= 0.85 else 'âš ï¸'}\")\n",
    "print(f\"   F1-Score: {stats.f1:.1%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f49a2",
   "metadata": {},
   "source": [
    "## 17. ğŸ”§ Náº¿u Recall váº«n chÆ°a Ä‘á»§ - Fine-tuning thÃªm\n",
    "\n",
    "**Chá»‰ cháº¡y cell nÃ y náº¿u Recall < 85%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra Recall hiá»‡n táº¡i\n",
    "if stats.recall < 0.85:\n",
    "    print(f\"âš ï¸ Current Recall: {stats.recall:.1%} < 85%\")\n",
    "    print(f\"\\nğŸ’¡ RECOMMENDATIONS TO IMPROVE RECALL:\\n\")\n",
    "    \n",
    "    print(\"1ï¸âƒ£ GIáº¢M prob_threshold thÃªm:\")\n",
    "    print(\"   Current:\", model.thresholds.prob)\n",
    "    print(\"   Try: 0.2, 0.25\")\n",
    "    print(\"   Code: model.thresholds.prob = 0.2\")\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ TÄ‚NG n_rays thÃªm:\")\n",
    "    print(\"   Current: 96\")\n",
    "    print(\"   Try: 128 (nhÆ°ng sáº½ cháº­m hÆ¡n)\")\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ TRAIN thÃªm 50 epochs:\")\n",
    "    print(\"   history2 = model.train(...)\")\n",
    "    \n",
    "    print(\"\\n4ï¸âƒ£ THá»¬ cÃ¡c threshold khÃ¡c nhau:\")\n",
    "    print(\"\")\n",
    "    for prob_th in [0.2, 0.25, 0.35]:\n",
    "        model.thresholds.prob = prob_th\n",
    "        Y_test = [model.predict_instances(img, n_tiles=model._guess_n_tiles(img))[0] \n",
    "                  for img in X_val_norm[:5]]\n",
    "        stats_test = matching_dataset(Y_val[:5], Y_test, thresh=0.5, show_progress=False)\n",
    "        print(f\"   prob={prob_th:.2f}: Precision={stats_test.precision:.3f}, \"\n",
    "              f\"Recall={stats_test.recall:.3f}, F1={stats_test.f1:.3f}\")\n",
    "else:\n",
    "    print(f\"âœ…âœ…âœ… RECALL TARGET ACHIEVED!\")\n",
    "    print(f\"   Current Recall: {stats.recall:.1%} >= 85%\")\n",
    "    print(f\"\\nğŸ‰ Model V3 hoÃ n thÃ nh má»¥c tiÃªu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0006dde",
   "metadata": {},
   "source": [
    "## 18. ğŸ“¦ Download model vá» mÃ¡y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32112125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NÃ©n model folder\n",
    "!zip -r stardist_v3_high_recall.zip models/stardist_v3_high_recall\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('stardist_v3_high_recall.zip')\n",
    "\n",
    "print(\"âœ… Model ready to download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433f976",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ SUMMARY - V3 HIGH RECALL\n",
    "\n",
    "### Key Improvements:\n",
    "1. âœ… **n_rays: 96** (â†‘50% from 64) â†’ Better shape detection\n",
    "2. âœ… **prob_thresh: 0.3** (â†“37% from 0.479) â†’ Less missed cells\n",
    "3. âœ… **150 epochs** (â†‘50% from 100) â†’ Better convergence\n",
    "4. âœ… **Enhanced augmentation** â†’ Scale + Noise\n",
    "5. âœ… **Longer patience** â†’ More training time\n",
    "\n",
    "### Expected Results:\n",
    "- ğŸ¯ **Recall: 85-90%** (tá»« 76.3%)\n",
    "- ğŸ¯ **Precision: 80-85%** (tá»« 81.7%)\n",
    "- ğŸ¯ **F1-Score: 82-87%** (tá»« 78.9%)\n",
    "\n",
    "### Next Steps:\n",
    "1. Download model vá» mÃ¡y\n",
    "2. Test trÃªn full dataset\n",
    "3. Náº¿u Recall váº«n < 85%, thá»­ prob_thresh = 0.2-0.25\n",
    "4. Monitor Precision - náº¿u < 75% thÃ¬ tÄƒng prob_thresh láº¡i"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
