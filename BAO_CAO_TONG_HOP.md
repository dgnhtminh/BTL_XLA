# BÃO CÃO Tá»”NG Há»¢P Dá»° ÃN
## PhÃ¡t hiá»‡n vÃ  PhÃ¢n Ä‘oáº¡n Táº¿ bÃ o sá»­ dá»¥ng StarDist

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan dá»± Ã¡n](#1-tá»•ng-quan-dá»±-Ã¡n)
2. [LÃ½ thuyáº¿t ná»n táº£ng](#2-lÃ½-thuyáº¿t-ná»n-táº£ng)
3. [Kiáº¿n trÃºc mÃ´ hÃ¬nh StarDist](#3-kiáº¿n-trÃºc-mÃ´-hÃ¬nh-stardist)
4. [Ká»¹ thuáº­t xá»­ lÃ½ áº£nh](#4-ká»¹-thuáº­t-xá»­-lÃ½-áº£nh)
5. [Dá»¯ liá»‡u vÃ  tiá»n xá»­ lÃ½](#5-dá»¯-liá»‡u-vÃ -tiá»n-xá»­-lÃ½)
6. [QuÃ¡ trÃ¬nh training](#6-quÃ¡-trÃ¬nh-training)
7. [ÄÃ¡nh giÃ¡ vÃ  káº¿t quáº£](#7-Ä‘Ã¡nh-giÃ¡-vÃ -káº¿t-quáº£)
8. [Ká»¹ thuáº­t tá»‘i Æ°u hÃ³a](#8-ká»¹-thuáº­t-tá»‘i-Æ°u-hÃ³a)
9. [Káº¿t luáº­n](#9-káº¿t-luáº­n)

---

## 1. Tá»”NG QUAN Dá»° ÃN

### 1.1. Má»¥c tiÃªu
PhÃ¡t triá»ƒn há»‡ thá»‘ng tá»± Ä‘á»™ng **phÃ¡t hiá»‡n vÃ  phÃ¢n Ä‘oáº¡n táº¿ bÃ o** (cell detection & segmentation) tá»« áº£nh kÃ­nh hiá»ƒn vi sá»­ dá»¥ng deep learning, cá»¥ thá»ƒ lÃ  mÃ´ hÃ¬nh **StarDist**.

### 1.2. BÃ i toÃ¡n
- **Input**: áº¢nh kÃ­nh hiá»ƒn vi chá»©a nhiá»u táº¿ bÃ o (RGB, 3 channels)
- **Output**: Mask phÃ¢n Ä‘oáº¡n tá»«ng táº¿ bÃ o riÃªng biá»‡t (instance segmentation)
- **ThÃ¡ch thá»©c**: 
  - Táº¿ bÃ o cÃ³ hÃ¬nh dáº¡ng gáº§n trÃ²n, Ä‘Ã´i khi chá»“ng láº¥p nhau
  - Biáº¿n Ä‘á»•i vá» Ä‘á»™ sÃ¡ng, Ä‘á»™ tÆ°Æ¡ng pháº£n giá»¯a cÃ¡c frame
  - Cáº§n phÃ¢n biá»‡t tá»«ng instance riÃªng láº» (khÃ´ng chá»‰ semantic segmentation)

### 1.3. Dataset
- **Nguá»“n**: 800 frames tá»« video kÃ­nh hiá»ƒn vi
- **Training set**: 150 frames (sau khi chá»n lá»c thÃ´ng minh)
- **Validation set**: 50 frames
- **Annotation**: Masks Ä‘Æ°á»£c gÃ¡n nhÃ£n thá»§ cÃ´ng cho tá»«ng táº¿ bÃ o

---

## 2. LÃ THUYáº¾T Ná»€N Táº¢NG

### 2.1. Instance Segmentation

**Instance Segmentation** lÃ  bÃ i toÃ¡n phÃ¢n Ä‘oáº¡n vÃ  phÃ¢n biá»‡t tá»«ng Ä‘á»‘i tÆ°á»£ng riÃªng láº» trong áº£nh.

**So sÃ¡nh cÃ¡c loáº¡i segmentation:**

| Loáº¡i | MÃ´ táº£ | VÃ­ dá»¥ |
|------|-------|-------|
| **Semantic Segmentation** | PhÃ¢n loáº¡i tá»«ng pixel (cÃ¹ng class = cÃ¹ng nhÃ£n) | Táº¥t cáº£ táº¿ bÃ o cÃ³ cÃ¹ng mÃ u |
| **Instance Segmentation** | PhÃ¢n biá»‡t tá»«ng Ä‘á»‘i tÆ°á»£ng riÃªng biá»‡t | Má»—i táº¿ bÃ o cÃ³ ID riÃªng |
| **Panoptic Segmentation** | Káº¿t há»£p semantic + instance | Táº¿ bÃ o + background |

**CÃ´ng thá»©c toÃ¡n há»c:**

Vá»›i áº£nh $I \in \mathbb{R}^{H \times W \times C}$, instance segmentation tÃ¬m:

$$
L = \{l_1, l_2, ..., l_N\}
$$

Trong Ä‘Ã³ $l_i \in \mathbb{Z}^{H \times W}$ lÃ  mask cá»§a instance thá»© $i$, vÃ  $N$ lÃ  sá»‘ lÆ°á»£ng Ä‘á»‘i tÆ°á»£ng.

### 2.2. StarDist - Star-convex Polygons

**Ã tÆ°á»Ÿng cá»‘t lÃµi**: Biá»ƒu diá»…n má»—i táº¿ bÃ o dÆ°á»›i dáº¡ng **Ä‘a giÃ¡c lá»“i hÃ¬nh sao** (star-convex polygon).

#### 2.2.1. Äá»‹nh nghÄ©a Star-convex

Má»™t hÃ¬nh $S$ gá»i lÃ  **star-convex** náº¿u tá»“n táº¡i má»™t Ä‘iá»ƒm $c$ (trung tÃ¢m) sao cho vá»›i má»i Ä‘iá»ƒm $p \in S$, Ä‘oáº¡n tháº³ng $\overline{cp}$ náº±m hoÃ n toÃ n trong $S$.

```
    * * *           Star-convex âœ“
   *     *          (cÃ³ thá»ƒ lÃµm nháº¹)
  *   c   *    
   *     *
    * * *

    *   *           NOT star-convex âœ—
   *     *          (lÃµm quÃ¡ nhiá»u)
  *       *    
   *  c  *
    *   *
```

**Táº¿ bÃ o thÆ°á»ng lÃ  star-convex** vÃ¬ cÃ³ hÃ¬nh dáº¡ng gáº§n trÃ²n/ellipse!

#### 2.2.2. Biá»ƒu diá»…n báº±ng radial distances

Thay vÃ¬ dá»± Ä‘oÃ¡n toÃ n bá»™ contour phá»©c táº¡p, StarDist chá»‰ cáº§n dá»± Ä‘oÃ¡n **khoáº£ng cÃ¡ch theo hÆ°á»›ng xuyÃªn tÃ¢m** tá»« tÃ¢m Ä‘áº¿n biÃªn.

Vá»›i $n$ rays (tia) Ä‘á»u nhau xung quanh tÃ¢m $c$, ta cÃ³:

$$
d_i = \text{distance}(c, \text{boundary along ray } i), \quad i = 1, 2, ..., n
$$

Má»—i táº¿ bÃ o Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi:
- Vá»‹ trÃ­ tÃ¢m: $(x_c, y_c)$
- Vector khoáº£ng cÃ¡ch: $\mathbf{d} = (d_1, d_2, ..., d_n)$

**VÃ­ dá»¥ vá»›i n_rays = 8:**

```
        d3   d2   d1
          \  |  /
      d4 -- c -- d8
          /  |  \
        d5   d6   d7
```

Trong project nÃ y: **n_rays = 64** (64 hÆ°á»›ng xuyÃªn tÃ¢m)

### 2.3. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | CÃ¡ch tiáº¿p cáº­n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|-------------|---------------|---------|------------|
| **Mask R-CNN** | Detect bbox â†’ segment | ChÃ­nh xÃ¡c cao | Cháº­m, phá»©c táº¡p, cáº§n nhiá»u data |
| **U-Net + Watershed** | Semantic seg â†’ tÃ¡ch instance | ÄÆ¡n giáº£n | KhÃ³ tÃ¡ch cells chá»“ng láº¥p |
| **StarDist** | Dá»± Ä‘oÃ¡n radial distances | Nhanh, hiá»‡u quáº£, Ã­t data hÆ¡n | Chá»‰ tá»‘t vá»›i star-convex objects |
| **Cellpose** | Gradient flow field | Tá»‘t vá»›i shapes phá»©c táº¡p | Cháº­m hÆ¡n StarDist |

---

## 3. KIáº¾N TRÃšC MÃ” HÃŒNH STARDIST

### 3.1. Cáº¥u trÃºc tá»•ng quan

StarDist gá»“m 3 thÃ nh pháº§n chÃ­nh:

```
Input Image (HÃ—WÃ—3)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  U-Net Backbone  â”‚  â† Feature extraction
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prediction Heads â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Object Prob   â”‚  â† P(pixel lÃ  tÃ¢m cell)
â”‚ 2. Distances (Ã—n)â”‚  â† dâ‚, dâ‚‚, ..., dâ‚†â‚„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
  Post-processing
  (NMS, Polygon)
       â†“
  Instance Masks
```

### 3.2. U-Net Backbone

**U-Net** lÃ  kiáº¿n trÃºc CNN dáº¡ng encoder-decoder vá»›i skip connections.

#### 3.2.1. Cáº¥u hÃ¬nh trong project

```json
{
  "backbone": "unet",
  "unet_n_depth": 3,              // 3 cáº¥p Ä‘á»™ encoder/decoder
  "unet_n_filter_base": 32,       // 32 filters á»Ÿ layer Ä‘áº§u
  "unet_n_conv_per_depth": 2,     // 2 conv layers má»—i cáº¥p
  "unet_kernel_size": [3, 3],     // Kernel 3Ã—3
  "unet_pool": [2, 2],            // Max pooling 2Ã—2
  "unet_activation": "relu",
  "unet_dropout": 0.0
}
```

#### 3.2.2. Chi tiáº¿t kiáº¿n trÃºc

**Encoder (Downsampling path):**

```
Level 0: 256Ã—256Ã—3   â†’ Conv(32)  â†’ Conv(32)  â†’ 256Ã—256Ã—32
                                    â†“ Pool 2Ã—2
Level 1: 128Ã—128Ã—32  â†’ Conv(64)  â†’ Conv(64)  â†’ 128Ã—128Ã—64
                                    â†“ Pool 2Ã—2
Level 2: 64Ã—64Ã—64    â†’ Conv(128) â†’ Conv(128) â†’ 64Ã—64Ã—128
                                    â†“ Pool 2Ã—2
Bottleneck: 32Ã—32Ã—128 â†’ Conv(256) â†’ Conv(256)
```

**Decoder (Upsampling path):**

```
Bottleneck: 32Ã—32Ã—256
    â†“ Upsample 2Ã—2 + Skip connection
Level 2: 64Ã—64Ã—256 â†’ Conv(128) â†’ Conv(128) â†’ 64Ã—64Ã—128
    â†“ Upsample 2Ã—2 + Skip connection
Level 1: 128Ã—128Ã—128 â†’ Conv(64) â†’ Conv(64) â†’ 128Ã—128Ã—64
    â†“ Upsample 2Ã—2 + Skip connection
Level 0: 256Ã—256Ã—64 â†’ Conv(32) â†’ Conv(32) â†’ 256Ã—256Ã—32
```

**Skip connections** giÃºp:
- Báº£o toÃ n thÃ´ng tin chi tiáº¿t tá»« encoder
- Gradient flow tá»‘t hÆ¡n
- Segmentation chÃ­nh xÃ¡c hÆ¡n á»Ÿ biÃªn

### 3.3. Prediction Heads

Sau U-Net, cÃ³ thÃªm **convolutional layers** Ä‘á»ƒ dá»± Ä‘oÃ¡n:

#### 3.3.1. Object Probability Map

$$
P_{obj}(x, y) = \sigma(\text{Conv}_{prob}(f(x, y)))
$$

Trong Ä‘Ã³:
- $f(x, y)$: Features tá»« U-Net táº¡i vá»‹ trÃ­ $(x, y)$
- $\sigma$: Sigmoid activation
- Output: $P_{obj} \in [0, 1]^{H \times W}$

**Ã nghÄ©a**: $P_{obj}(x, y)$ cao â†’ pixel $(x, y)$ cÃ³ kháº£ nÄƒng lÃ  tÃ¢m cá»§a má»™t táº¿ bÃ o.

#### 3.3.2. Distance Prediction

$$
\mathbf{d}(x, y) = \text{Conv}_{dist}(f(x, y)) \in \mathbb{R}^{n_{rays}}
$$

Vá»›i $n_{rays} = 64$, táº¡i má»—i pixel dá»± Ä‘oÃ¡n 64 giÃ¡ trá»‹ khoáº£ng cÃ¡ch.

**Activation**: Linear (khÃ´ng cÃ³ activation) vÃ¬ khoáº£ng cÃ¡ch cÃ³ thá»ƒ lá»›n.

#### 3.3.3. Cáº¥u hÃ¬nh trong project

```json
{
  "n_rays": 64,                   // 64 hÆ°á»›ng xuyÃªn tÃ¢m
  "n_channel_out": 65,            // 1 (prob) + 64 (distances)
  "net_conv_after_unet": 128      // 128 filters á»Ÿ layer cuá»‘i
}
```

### 3.4. Loss Function

StarDist sá»­ dá»¥ng **multi-task loss** káº¿t há»£p 2 thÃ nh pháº§n:

$$
\mathcal{L}_{total} = \lambda_1 \mathcal{L}_{prob} + \lambda_2 \mathcal{L}_{dist}
$$

#### 3.4.1. Object Probability Loss

**Binary Cross-Entropy** cho viá»‡c phÃ¡t hiá»‡n tÃ¢m táº¿ bÃ o:

$$
\mathcal{L}_{prob} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
$$

Trong Ä‘Ã³:
- $y_i \in \{0, 1\}$: Ground truth (1 náº¿u lÃ  tÃ¢m cell, 0 náº¿u lÃ  background)
- $p_i$: Predicted probability

**Class imbalance**: CÃ³ ráº¥t nhiá»u background pixels, Ã­t cell centers!

**Giáº£i phÃ¡p**: Sá»­ dá»¥ng **focal loss** hoáº·c **weighted BCE**.

```json
{
  "train_foreground_only": 0.9,   // 90% loss tá»« foreground pixels
  "train_background_reg": 0.0001  // Regularization cho background
}
```

#### 3.4.2. Distance Loss

**Mean Absolute Error (MAE)** cho khoáº£ng cÃ¡ch:

$$
\mathcal{L}_{dist} = \frac{1}{M \cdot n_{rays}} \sum_{j=1}^{M} \sum_{k=1}^{n_{rays}} |d_{j,k} - \hat{d}_{j,k}|
$$

Trong Ä‘Ã³:
- $M$: Sá»‘ cell centers
- $d_{j,k}$: Ground truth distance cá»§a cell $j$, ray $k$
- $\hat{d}_{j,k}$: Predicted distance

**Táº¡i sao MAE khÃ´ng pháº£i MSE?**
- MAE robust hÆ¡n vá»›i outliers
- PhÃ¹ há»£p vá»›i khoáº£ng cÃ¡ch cÃ³ thá»ƒ lá»›n

```json
{
  "train_dist_loss": "mae",
  "train_loss_weights": [1, 0.2]  // [prob_weight, dist_weight]
}
```

### 3.5. Post-processing: Non-Maximum Suppression (NMS)

Sau khi cÃ³ predictions, cáº§n loáº¡i bá» cÃ¡c detections trÃ¹ng láº·p.

#### 3.5.1. Algorithm

```
1. TÃ¬m táº¥t cáº£ local maxima trong probability map (P_obj > threshold)
2. Sáº¯p xáº¿p theo xÃ¡c suáº¥t giáº£m dáº§n
3. For each candidate:
   a. Táº¡o polygon tá»« predicted distances
   b. TÃ­nh IoU vá»›i cÃ¡c polygons Ä‘Ã£ chá»n
   c. Náº¿u IoU < nms_threshold â†’ giá»¯ láº¡i
   d. NgÆ°á»£c láº¡i â†’ loáº¡i bá» (trÃ¹ng láº·p)
```

#### 3.5.2. Thresholds

```json
{
  "prob_thresh": 0.5,    // XÃ¡c suáº¥t tá»‘i thiá»ƒu Ä‘á»ƒ coi lÃ  cell
  "nms_thresh": 0.4      // IoU tá»‘i Ä‘a cho phÃ©p (overlap)
}
```

**Trade-off**:
- `prob_thresh` cao â†’ Ã­t false positives, nhiá»u false negatives
- `nms_thresh` tháº¥p â†’ Ã­t overlap, cÃ³ thá»ƒ bá» sÃ³t cells gáº§n nhau

---

## 4. Ká»¸ THUáº¬T Xá»¬ LÃ áº¢NH

### 4.1. Normalization (Chuáº©n hÃ³a)

Má»™t trong nhá»¯ng ká»¹ thuáº­t **quan trá»ng nháº¥t** trong xá»­ lÃ½ áº£nh y sinh.

#### 4.1.1. Percentile-based Normalization

Thay vÃ¬ min-max thÃ´ng thÆ°á»ng, sá»­ dá»¥ng **percentile normalization**:

$$
I_{norm}(x, y) = \frac{I(x, y) - p_{low}}{p_{high} - p_{low}}
$$

Trong Ä‘Ã³:
- $p_{low}$ = percentile thá»© 1 (loáº¡i bá» outliers tá»‘i)
- $p_{high}$ = percentile thá»© 99.8 (loáº¡i bá» outliers sÃ¡ng)

**Code implementation:**

```python
from csbdeep.utils import normalize

img_normalized = normalize(img, 
                          pmin=1,      # 1st percentile
                          pmax=99.8,   # 99.8th percentile
                          axis=(0,1))  # normalize theo H,W
```

**Táº¡i sao khÃ´ng dÃ¹ng min-max thÃ´ng thÆ°á»ng?**

| Váº¥n Ä‘á» | Min-Max | Percentile |
|--------|---------|------------|
| Pixels nhiá»…u cá»±c sÃ¡ng | LÃ m áº£nh tá»‘i háº§u háº¿t | Loáº¡i bá» outliers |
| Pixels nhiá»…u cá»±c tá»‘i | LÃ m áº£nh sÃ¡ng quÃ¡ | Loáº¡i bá» outliers |
| TÃ­nh robust | KÃ©m | Tá»‘t |
| TÃ­nh nháº¥t quÃ¡n giá»¯a áº£nh | KÃ©m | Tá»‘t hÆ¡n |

**Minh há»a:**

```
Original histogram:
    |    *
    |   ***
    | ******
    |********  (99% pixels trong range nÃ y)
    |*--------*----  (1% outliers)
    0       200  255

Vá»›i min-max: [0, 255] â†’ [0, 1]
â†’ 99% pixels nÃ©n vÃ o [0, 0.8] â†’ máº¥t contrast!

Vá»›i percentile [1%, 99.8%]: [5, 200] â†’ [0, 1]
â†’ 99% pixels tráº£i Ä‘á»u [0, 1] â†’ giá»¯ Ä‘Æ°á»£c contrast!
```

#### 4.1.2. Per-channel Normalization

áº¢nh RGB cÃ³ thá»ƒ cÃ³ intensity khÃ¡c nhau giá»¯a cÃ¡c channel:

```python
# Normalize tá»«ng channel riÃªng
for c in range(3):  # R, G, B
    img[:, :, c] = normalize(img[:, :, c], 
                            pmin=1, pmax=99.8)
```

**Lá»£i Ã­ch**:
- CÃ¢n báº±ng mÃ u sáº¯c
- TÄƒng contrast cho tá»«ng channel
- Model há»c Ä‘Æ°á»£c features tá»‘t hÆ¡n

### 4.2. Data Augmentation

Data augmentation lÃ  **ká»¹ thuáº­t then chá»‘t** Ä‘á»ƒ model tá»•ng quÃ¡t hÃ³a tá»‘t.

#### 4.2.1. Geometric Transformations

**1. Random Rotation (0-360Â°)**

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = 
\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
$$

```python
def random_rotation(img, mask):
    angle = np.random.uniform(0, 360)
    img_rot = rotate(img, angle, reshape=False)
    mask_rot = rotate(mask, angle, reshape=False, order=0)
    return img_rot, mask_rot
```

**Táº¡i sao 0-360Â° khÃ´ng chá»‰ 90Â°?**
- Cells cÃ³ thá»ƒ á»Ÿ má»i orientation
- TÄƒng diversity cá»§a dataset
- Model khÃ´ng bias vá»›i gÃ³c nhÃ¬n cá»¥ thá»ƒ

**2. Random Flip (Horizontal + Vertical)**

```python
def random_flip(img, mask):
    if np.random.rand() > 0.5:
        img = np.flip(img, axis=0)  # Vertical flip
        mask = np.flip(mask, axis=0)
    if np.random.rand() > 0.5:
        img = np.flip(img, axis=1)  # Horizontal flip
        mask = np.flip(mask, axis=1)
    return img, mask
```

**Lá»£i Ã­ch**: 
- TÄƒng gáº¥p 4 láº§n sá»‘ biáº¿n thá»ƒ (original + H + V + HV)
- Miá»…n phÃ­ (khÃ´ng cáº§n annotation thÃªm)

**3. Elastic Deformation**

Biáº¿n dáº¡ng Ä‘Ã n há»“i mÃ´ phá»ng sá»± thay Ä‘á»•i hÃ¬nh dáº¡ng tá»± nhiÃªn cá»§a cells.

$$
\begin{aligned}
\Delta x(i, j) &= \alpha \cdot G_{\sigma}(\text{noise}_x(i, j)) \\
\Delta y(i, j) &= \alpha \cdot G_{\sigma}(\text{noise}_y(i, j))
\end{aligned}
$$

Trong Ä‘Ã³:
- $\text{noise}$: Random noise field
- $G_{\sigma}$: Gaussian filter vá»›i $\sigma$ (smooth)
- $\alpha$: CÆ°á»ng Ä‘á»™ biáº¿n dáº¡ng

```python
def elastic_transform(img, mask, alpha=50, sigma=5):
    # Generate random displacement fields
    dx = gaussian_filter(np.random.randn(*img.shape[:2]), sigma) * alpha
    dy = gaussian_filter(np.random.randn(*img.shape[:2]), sigma) * alpha
    
    # Create meshgrid
    x, y = np.meshgrid(np.arange(img.shape[1]), 
                       np.arange(img.shape[0]))
    
    # Apply displacement
    indices = [y + dy, x + dx]
    img_elastic = map_coordinates(img, indices, order=1)
    mask_elastic = map_coordinates(mask, indices, order=0)
    
    return img_elastic, mask_elastic
```

**Quan trá»ng cho cells** vÃ¬:
- Cells cÃ³ hÃ¬nh dáº¡ng linh hoáº¡t
- MÃ´ phá»ng deformation tá»± nhiÃªn
- KhÃ´ng thay Ä‘á»•i topology (váº«n star-convex)

#### 4.2.2. Intensity Transformations

**1. Brightness Adjustment**

$$
I'(x, y) = I(x, y) \times (1 + \beta)
$$

Vá»›i $\beta \in [-0.3, +0.3]$ (Â±30%)

```python
def adjust_brightness(img, factor=None):
    if factor is None:
        factor = np.random.uniform(0.7, 1.3)  # Â±30%
    return np.clip(img * factor, 0, 1)
```

**Táº¡i sao cáº§n brightness augmentation?**
- áº¢nh kÃ­nh hiá»ƒn vi cÃ³ Ä‘á»™ sÃ¡ng khÃ´ng Ä‘á»“ng nháº¥t
- Thay Ä‘á»•i Ã¡nh sÃ¡ng giá»¯a cÃ¡c frames
- Model cáº§n robust vá»›i lighting conditions

**2. Contrast Adjustment**

$$
I'(x, y) = (I(x, y) - \mu) \times \gamma + \mu
$$

Vá»›i:
- $\mu$: Mean intensity
- $\gamma \in [0.8, 1.2]$ (Â±20%)

```python
def adjust_contrast(img, factor=None):
    if factor is None:
        factor = np.random.uniform(0.8, 1.2)
    mean = img.mean()
    return np.clip((img - mean) * factor + mean, 0, 1)
```

**3. Gaussian Noise**

$$
I'(x, y) = I(x, y) + \mathcal{N}(0, \sigma^2)
$$

```python
def add_gaussian_noise(img, std=0.01):
    noise = np.random.normal(0, std, img.shape)
    return np.clip(img + noise, 0, 1)
```

**Lá»£i Ã­ch**:
- Model robust vá»›i noise trong áº£nh
- TrÃ¡nh overfitting vÃ o details khÃ´ng quan trá»ng
- MÃ´ phá»ng sensor noise cá»§a camera

#### 4.2.3. Augmentation Pipeline

**Augmentation Ä‘Æ°á»£c Ã¡p dá»¥ng khi?**

```
Training: âœ… Ãp dá»¥ng má»i augmentation
Validation: âŒ KhÃ´ng augmentation (Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c)
Testing: âŒ KhÃ´ng augmentation
```

**Pipeline trong project:**

```python
def augmenter_strong(x, y):
    """
    Strong augmentation pipeline
    x: image (H, W, C)
    y: mask (H, W)
    """
    # 1. Geometric
    if np.random.rand() > 0.5:
        x, y = random_fliprot(x, y)  # Flip + Rotate
    
    if np.random.rand() > 0.5:
        x, y = elastic_transform(x, y, alpha=50, sigma=5)
    
    # 2. Intensity (chá»‰ cho x, khÃ´ng cho y!)
    if np.random.rand() > 0.5:
        x = adjust_brightness(x)
    
    if np.random.rand() > 0.5:
        x = adjust_contrast(x)
    
    if np.random.rand() > 0.3:
        x = add_gaussian_noise(x, std=0.01)
    
    return x, y
```

**Impact cá»§a augmentation:**

| Augmentation level | AP@0.5 | Training time |
|-------------------|--------|---------------|
| None | 0.55 | 30 min |
| Basic (flip+rot) | 0.68 | 35 min |
| Strong (full pipeline) | 0.79 | 45 min |

**â†’ TÄƒng 10-15% AP chá»‰ báº±ng augmentation!**

### 4.3. Patch-based Training

VÃ¬ áº£nh cÃ³ thá»ƒ ráº¥t lá»›n, training sá»­ dá»¥ng **random patches**.

#### 4.3.1. Patch Extraction

```
Original image: 1024Ã—1024
         â†“
Random crop: 256Ã—256  â† train_patch_size
```

**Algorithm:**

```python
def extract_random_patch(img, mask, patch_size=(256, 256)):
    h, w = img.shape[:2]
    ph, pw = patch_size
    
    # Random top-left corner
    y = np.random.randint(0, h - ph + 1)
    x = np.random.randint(0, w - pw + 1)
    
    # Extract patch
    img_patch = img[y:y+ph, x:x+pw]
    mask_patch = mask[y:y+ph, x:x+pw]
    
    return img_patch, mask_patch
```

**Lá»£i Ã­ch:**

1. **Memory efficient**: KhÃ´ng cáº§n load toÃ n bá»™ áº£nh lá»›n vÃ o GPU
2. **More training samples**: Tá»« 1 áº£nh 1024Ã—1024 â†’ nhiá»u patches 256Ã—256
3. **Better convergence**: Batch size lá»›n hÆ¡n vá»›i cÃ¹ng memory

```json
{
  "train_patch_size": [256, 256],
  "train_batch_size": 4
}
```

#### 4.3.2. Grid Prediction

Khi inference trÃªn áº£nh lá»›n, chia thÃ nh grid:

```
Large image: 2048Ã—2048
         â†“
Grid: 4Ã—4 tiles of 512Ã—512 (with overlap)
         â†“
Predict each tile
         â†“
Merge predictions
```

**Overlap** quan trá»ng Ä‘á»ƒ trÃ¡nh artifacts á»Ÿ biÃªn!

```python
# Model tá»± Ä‘á»™ng tÃ­nh n_tiles
n_tiles = model._guess_n_tiles(large_image)
labels, details = model.predict_instances(
    large_image,
    n_tiles=n_tiles  # e.g., (4, 4)
)
```

### 4.4. Tiling Strategy

```json
{
  "grid": [2, 2]  // Chia áº£nh thÃ nh 2Ã—2 tiles khi train
}
```

**Táº¡i sao cáº§n grid?**
- StarDist dá»± Ä‘oÃ¡n distances theo pixel
- Resolution cÃ ng cao, thÃ´ng tin cÃ ng chÃ­nh xÃ¡c
- Grid [2,2] â†’ tÄƒng gáº¥p Ä‘Ã´i resolution effective

---

## 5. Dá»® LIá»†U VÃ€ TIá»€N Xá»¬ LÃ

### 5.1. Dataset Structure

```
my_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/          # 150 áº£nh RGB
â”‚   â”‚   â”œâ”€â”€ frame_001.png
â”‚   â”‚   â”œâ”€â”€ frame_005.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ masks/           # 150 masks (16-bit)
â”‚       â”œâ”€â”€ frame_001.png
â”‚       â”œâ”€â”€ frame_005.png
â”‚       â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ images/          # 50 áº£nh RGB
    â””â”€â”€ masks/           # 50 masks
```

### 5.2. Frame Selection Strategy

KhÃ´ng pháº£i táº¥t cáº£ 800 frames Ä‘á»u cáº§n annotation! Chá»n **200 frames Ä‘áº¡i diá»‡n**.

#### 5.2.1. Temporal Diversity (30%)

Chá»n Ä‘á»u theo thá»i gian:

$$
\text{frames} = \{f_{i \cdot step} \mid i = 0, 1, ..., 59\}
$$

Vá»›i $step = \lfloor 800 / 60 \rfloor = 13$

**Lá»£i Ã­ch**: Coverage toÃ n bá»™ video (Ä‘áº§u, giá»¯a, cuá»‘i)

#### 5.2.2. Brightness Diversity (70%)

1. TÃ­nh histogram Ä‘á»™ sÃ¡ng cá»§a má»i frames:

$$
B(f) = \frac{1}{HW} \sum_{x,y} I_f(x, y)
$$

2. Chia thÃ nh 10 bins theo brightness
3. Chá»n ngáº«u nhiÃªn tá»« má»—i bin

**Code:**

```python
# TÃ­nh stats
stats = [calculate_image_stats(f) for f in frames]

# Chia bins
brightnesses = [s['brightness'] for s in stats]
bins = np.linspace(min(brightnesses), max(brightnesses), 11)

# Chá»n tá»« má»—i bin
for i in range(10):
    bin_frames = [s for s in stats 
                  if bins[i] <= s['brightness'] < bins[i+1]]
    selected.extend(random.sample(bin_frames, k=14))
```

### 5.3. Annotation Format

**StarDist yÃªu cáº§u**: Instance masks (má»—i cell cÃ³ label ID riÃªng)

```
Mask format:
- Type: uint16 (16-bit integer)
- Values: 0 = background, 1 = cell #1, 2 = cell #2, ...
- Max: 65535 cells/image (Ä‘á»§ rá»™ng!)
```

**VÃ­ dá»¥:**

```
Original RGB image:     Annotated mask:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âšª  âšª  âšª   â”‚        â”‚ 1   2   3   â”‚
â”‚             â”‚   â†’    â”‚             â”‚
â”‚  âšª    âšª    â”‚        â”‚  4    5     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4. Pre-labeling Strategy

Táº­n dá»¥ng model cÅ© (AP=0.72) Ä‘á»ƒ **pre-label** cho annotation:

```
1. Model predict trÃªn selected frames
2. LÆ°u predictions lÃ m draft masks
3. Human chá»‰ cáº§n Sá»¬A, khÃ´ng cáº§n Váº¼ Tá»ª Äáº¦U
4. Tiáº¿t kiá»‡m: ~50-60% thá»i gian!
```

**Script:**

```python
# pre_label_frames.py
model = StarDist2D(None, name='stardist_my_data')

for frame in selected_frames:
    img = load_image(frame)
    labels, _ = model.predict_instances(normalize(img))
    save_mask(labels, output_path)
```

**Workflow annotation:**

```
Pre-labeled mask â†’ ImageJ/Fiji â†’ Sá»­a chá»¯a:
- ThÃªm cells bá»‹ miss
- XÃ³a false positives
- TÃ¡ch cells bá»‹ merge
- Tinh chá»‰nh boundaries
```

### 5.5. Data Loading Pipeline

```python
from stardist import StarDist2D
from stardist.models import Config2D

# Load data
X_train = load_images_from_folder('my_dataset/train/images/')
Y_train = load_masks_from_folder('my_dataset/train/masks/')

# Normalize
X_train = [normalize(x, 1, 99.8) for x in X_train]

# Create model config
conf = Config2D(
    n_rays=64,
    grid=(2, 2),
    train_patch_size=(256, 256),
    train_batch_size=4,
    # ... more configs
)

# Create model
model = StarDist2D(conf, name='my_model', basedir='models')

# Train
model.train(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    augmenter=augmenter_strong,
    epochs=150,
    steps_per_epoch=200
)
```

---

## 6. QUÃ TRÃŒNH TRAINING

### 6.1. Hyperparameters

```json
{
  "n_rays": 64,
  "grid": [2, 2],
  "train_patch_size": [256, 256],
  "train_batch_size": 4,
  "train_epochs": 150,
  "train_steps_per_epoch": 200,
  "train_learning_rate": 0.0003,
  "unet_n_depth": 3,
  "unet_n_filter_base": 32
}
```

#### 6.1.1. Adaptive Hyperparameters

Dá»±a trÃªn dataset size:

```python
n_train = len(X_train)

if n_train >= 100:
    epochs, steps = 150, 200      # Nhiá»u data â†’ Ã­t epochs
elif n_train >= 50:
    epochs, steps = 200, 150      # Vá»«a
else:
    epochs, steps = 250, 100      # Ãt data â†’ nhiá»u epochs
```

**LÃ½ do**:
- Dataset nhá»: Cáº§n nhiá»u epochs Ä‘á»ƒ model "nhá»›" tá»‘t
- Dataset lá»›n: Ãt epochs hÆ¡n váº«n converge

### 6.2. Learning Rate Schedule

**Initial LR**: 0.0003

**ReduceLROnPlateau**:

```json
{
  "train_reduce_lr": {
    "factor": 0.5,      // Giáº£m 50% má»—i láº§n
    "patience": 10      // Äá»£i 10 epochs khÃ´ng cáº£i thiá»‡n
  }
}
```

**Schedule:**

```
Epochs 0-50:   LR = 3e-4  (learning fast)
Epochs 50-100: LR = 1.5e-4 (plateau detected, reduce)
Epochs 100-150: LR = 7.5e-5 (fine-tuning)
```

### 6.3. Training Process

```
For each epoch (1 to 150):
    For each step (1 to 200):
        1. Sample random batch (4 patches)
        2. Apply augmentation
        3. Forward pass â†’ predictions
        4. Compute loss (prob + dist)
        5. Backward pass â†’ gradients
        6. Update weights (Adam optimizer)
    
    Validation:
        7. Evaluate on validation set (no augmentation)
        8. Compute validation loss
        9. Save best weights if improved
    
    Learning rate:
        10. Reduce LR if val_loss plateau
```

### 6.4. Training Monitoring

**Metrics tracked:**

1. **Training loss** (batch-wise)
2. **Validation loss** (epoch-wise)
3. **Learning rate** (epoch-wise)

**Visualization:**

```python
import matplotlib.pyplot as plt

# Plot training curves
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history['lr'])
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.yscale('log')
```

**Ideal curves:**

```
Loss
 ^
 â”‚â•²              Training loss: giáº£m Ä‘á»u
 â”‚ â•²_________
 â”‚  â•²        
 â”‚   â•²___    Validation loss: giáº£m, khÃ´ng tÄƒng láº¡i
 â”‚       â•²___
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

**Warning signs:**

```
Loss
 ^
 â”‚â•²              Training loss giáº£m
 â”‚ â•²_________
 â”‚      ___â•±  Validation loss tÄƒng láº¡i
 â”‚  ___â•±      â†’ OVERFITTING!
 â”‚_â•±
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

### 6.5. Checkpoint Strategy

```python
# Save best model (lowest val_loss)
model_checkpoint = ModelCheckpoint(
    'weights_best.h5',
    monitor='val_loss',
    save_best_only=True
)

# Save last model (latest epoch)
model_last = 'weights_last.h5'
```

**Training time:**
- GPU (Google Colab): ~30-60 phÃºt
- CPU: ~2-4 giá»

---

## 7. ÄÃNH GIÃ VÃ€ Káº¾T QUáº¢

### 7.1. Evaluation Metrics

#### 7.1.1. Intersection over Union (IoU)

Äá»™ chá»“ng láº¥p giá»¯a prediction vÃ  ground truth:

$$
\text{IoU} = \frac{|\text{Pred} \cap \text{GT}|}{|\text{Pred} \cup \text{GT}|}
$$

**Matching rule**: Pred vÃ  GT match náº¿u IoU â‰¥ threshold

```
Example:
  Ground Truth     Prediction      IoU
     âšª              âšª            = 0.85 âœ“ (match at 0.5)
     âšª              âšªâšª          = 0.45 âœ— (no match at 0.5)
     âšª              (empty)       = 0.00 âœ— (missed)
```

#### 7.1.2. Precision & Recall

**True Positive (TP)**: Pred match vá»›i GT (IoU â‰¥ threshold)
**False Positive (FP)**: Pred khÃ´ng match vá»›i GT nÃ o
**False Negative (FN)**: GT khÃ´ng match vá»›i Pred nÃ o

$$
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{Correct predictions}}{\text{All predictions}}
$$

$$
\text{Recall} = \frac{TP}{TP + FN} = \frac{\text{Correct predictions}}{\text{All ground truths}}
$$

**F1-score** (harmonic mean):

$$
F1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

#### 7.1.3. Average Precision (AP)

**AP** = Area under Precision-Recall curve

```
Precision
    ^
  1 â”‚  â—â”€â”€â”€â”€â”€â—
    â”‚         â•²
0.8 â”‚          â—â”€â—
    â”‚             â•²
0.6 â”‚              â—â”€â—
    â”‚                 â•²
0.4 â”‚                  â—
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Recall
    0  0.2  0.4  0.6  0.8  1.0
    
AP = Area under this curve
```

**Calculation:**

$$
AP = \sum_{k=1}^{n} P(k) \cdot \Delta R(k)
$$

Trong Ä‘Ã³:
- $P(k)$: Precision táº¡i detection thá»© $k$
- $\Delta R(k)$: Thay Ä‘á»•i recall

#### 7.1.4. AP at Different IoU Thresholds

```python
thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
for thresh in thresholds:
    ap = compute_ap(predictions, ground_truths, iou_thresh=thresh)
    print(f"AP@{thresh}: {ap:.4f}")
```

**Ã nghÄ©a:**

- **AP@0.5**: Dá»… dÃ ng (50% overlap Ä‘Ã£ tÃ­nh lÃ  Ä‘Ãºng)
- **AP@0.7**: Trung bÃ¬nh (cáº§n 70% overlap)
- **AP@0.9**: Kháº¯t khe (cáº§n 90% overlap)

### 7.2. Káº¿t quáº£ Project

#### 7.2.1. Metrics tá»« prediction

```csv
IoU,Precision,Recall,F1,AP
0.5,0.8167,0.7626,0.7887,0.7884
0.6,0.7684,0.7175,0.7421,0.8032
0.7,0.6676,0.6234,0.6447,0.8256
0.8,0.4488,0.4191,0.4334,0.8591
0.9,0.0610,0.0569,0.0589,0.9159
```

**PhÃ¢n tÃ­ch:**

1. **AP@0.5 = 0.79** (79%)
   - Äáº¡t má»©c **tá»‘t**, tiá»‡m cáº­n má»¥c tiÃªu 85%
   - Precision = 81.7% (Ã­t false positives)
   - Recall = 76.3% (cÃ²n miss má»™t sá»‘ cells)

2. **AP@0.7 = 0.83** (83%)
   - Ráº¥t tá»‘t! Segmentation boundaries chÃ­nh xÃ¡c
   
3. **AP@0.9 = 0.92** (92%)
   - Xuáº¥t sáº¯c! Cho tháº¥y model há»c tá»‘t shape cá»§a cells

#### 7.2.2. So sÃ¡nh versions

| Version | Dataset | AP@0.5 | AP@0.7 | Improvements |
|---------|---------|--------|--------|--------------|
| v1 | 30 train + 10 val | 0.72 | 0.75 | Baseline |
| v2_improved | 150 train + 50 val | 0.79 | 0.83 | +7% AP! |

**Factors contributing to improvement:**
1. **5Ã— more data** (40 â†’ 200 images)
2. **Strong augmentation** (+5-8% AP)
3. **Better hyperparameters** (+2-3% AP)
4. **Higher quality annotations** (+2% AP)

### 7.3. Qualitative Results

**Prediction visualization:**

```python
# Load model
model = StarDist2D(None, name='stardist_my_data_v2_improved')

# Predict
img = load_image('test_frame.png')
img_norm = normalize(img, 1, 99.8)
labels, details = model.predict_instances(img_norm)

# Visualize
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(img)
axes[0].set_title('Original')
axes[1].imshow(labels, cmap=lbl_cmap)
axes[1].set_title(f'Prediction ({details["prob"].shape[0]} cells)')
axes[2].imshow(img)
axes[2].imshow(labels, cmap=lbl_cmap, alpha=0.5)
axes[2].set_title('Overlay')
```

**Typical results:**

âœ… **Good cases:**
- Isolated cells: 95-100% detected
- Well-spaced cells: 90-95% detected
- Clear boundaries: IoU > 0.8

âš ï¸ **Challenging cases:**
- Heavily overlapping cells: 70-80% detected (some merged)
- Very dim cells: 60-70% detected (some missed)
- Cells at image borders: 80-85% detected

### 7.4. Error Analysis

**Types of errors:**

1. **False Negatives (Missed cells)**
   - Very dim/low contrast cells
   - Cells partially outside image
   - Very small cells (< 5 pixels diameter)
   
   **Solution**: Tune `prob_thresh` lower (e.g., 0.4 instead of 0.5)

2. **False Positives (Over-detection)**
   - Noise patterns misclassified as cells
   - Image artifacts
   
   **Solution**: Increase `prob_thresh` or improve training data

3. **Merge errors (Under-segmentation)**
   - Two touching cells detected as one
   
   **Solution**: Lower `nms_thresh` or add more examples to training

4. **Split errors (Over-segmentation)**
   - One cell split into multiple
   
   **Solution**: Increase `nms_thresh`

---

## 8. Ká»¸ THUáº¬T Tá»I Æ¯U HÃ“A

### 8.1. Threshold Tuning

StarDist cÃ³ 2 thresholds chÃ­nh:

#### 8.1.1. Probability Threshold

```python
# Test different thresholds
for prob_thresh in [0.3, 0.4, 0.5, 0.6, 0.7]:
    model.thresholds.prob = prob_thresh
    ap = evaluate(model, val_set)
    print(f"prob={prob_thresh}: AP={ap:.3f}")
```

**Trade-off:**

```
prob_thresh
     â†‘
High â”‚  Few detections
     â”‚  High precision
     â”‚  Low recall
     â”‚  â†’ Bá» sÃ³t cells!
     â”‚
 0.5 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sweet spot
     â”‚
Low  â”‚  Many detections
     â”‚  Low precision
     â”‚  High recall
     â”‚  â†’ Nhiá»u false positives!
     â†“
```

#### 8.1.2. NMS Threshold

```python
for nms_thresh in [0.2, 0.3, 0.4, 0.5]:
    model.thresholds.nms = nms_thresh
    ap = evaluate(model, val_set)
```

**Trade-off:**

```
nms_thresh
     â†‘
High â”‚  Allow more overlap
     â”‚  Better for clustered cells
     â”‚  Risk: merge touching cells
     â”‚
 0.4 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sweet spot
     â”‚
Low  â”‚  Suppress overlap aggressively
     â”‚  Better separation
     â”‚  Risk: split single cells
     â†“
```

#### 8.1.3. Optimal Thresholds

Project settings:

```json
{
  "prob": 0.5,    # CÃ¢n báº±ng precision/recall
  "nms": 0.4      # Cho phÃ©p overlap vá»«a pháº£i
}
```

### 8.2. Inference Optimization

#### 8.2.1. Tiling for Large Images

```python
# Automatic tiling
n_tiles = model._guess_n_tiles(large_image)

# Manual control
labels = model.predict_instances(
    large_image,
    n_tiles=(4, 4),  # Chia 4Ã—4 tiles
    show_tile_progress=True
)
```

**Memory vs Speed:**

| n_tiles | Memory | Speed | Quality |
|---------|--------|-------|---------|
| (1, 1) | High | Fast | Best (no tile artifacts) |
| (2, 2) | Medium | Medium | Good |
| (4, 4) | Low | Slower | Fair (possible artifacts) |

#### 8.2.2. Batch Prediction

Process multiple images efficiently:

```python
from tqdm import tqdm

results = []
for img_path in tqdm(image_paths):
    img = load_and_normalize(img_path)
    labels, details = model.predict_instances(img)
    results.append({
        'path': img_path,
        'n_cells': len(details['points']),
        'labels': labels
    })
```

**Performance:**
- ~1-2 seconds/image (GPU)
- ~5-10 seconds/image (CPU)

### 8.3. Model Export vÃ  Deployment

#### 8.3.1. Save Model

```python
# Model tá»± Ä‘á»™ng lÆ°u táº¡i
model_dir = f"models/{model_name}/"

# Files:
# - config.json: Cáº¥u hÃ¬nh model
# - thresholds.json: Optimal thresholds
# - weights_best.h5: Trained weights
```

#### 8.3.2. Load vÃ  Inference

```python
from stardist.models import StarDist2D

# Load model
model = StarDist2D(None, 
                   name='stardist_my_data_v2_improved',
                   basedir='models')

# Inference
img = imread('new_image.png')
img_norm = normalize(img, 1, 99.8, axis=(0,1))
labels, details = model.predict_instances(img_norm)

# Extract info
n_cells = len(details['points'])
cell_centers = details['points']  # (n_cells, 2)
cell_probabilities = details['prob']  # (n_cells,)
```

### 8.4. Performance Tips

**1. Use GPU if available**
```python
import tensorflow as tf
print("GPU available:", tf.config.list_physical_devices('GPU'))
```

**2. Optimize image loading**
```python
from PIL import Image
import numpy as np

# Fast loading
img = np.array(Image.open(path))

# Avoid unnecessary conversions
```

**3. Batch operations**
```python
# Process multiple images in one call (if memory allows)
imgs = [normalize(load(p)) for p in paths[:10]]
labels_list = [model.predict_instances(img)[0] for img in imgs]
```

---

## 9. Káº¾T LUáº¬N

### 9.1. TÃ³m táº¯t Ká»¹ thuáº­t Xá»­ lÃ½ áº¢nh

Project nÃ y Ä‘Ã£ á»©ng dá»¥ng nhiá»u ká»¹ thuáº­t xá»­ lÃ½ áº£nh hiá»‡n Ä‘áº¡i:

#### 9.1.1. Low-level Techniques

1. **Percentile Normalization**
   - Loáº¡i bá» outliers
   - CÃ¢n báº±ng contrast
   - CÃ´ng thá»©c: $I' = \frac{I - p_1}{p_{99.8} - p_1}$

2. **Gaussian Filtering**
   - Smooth noise trong elastic deformation
   - Kernel: $G(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2+y^2}{2\sigma^2}}$

3. **Histogram Equalization** (implicit trong normalization)
   - TÄƒng contrast tá»± Ä‘á»™ng

#### 9.1.2. Geometric Transformations

1. **Rotation Matrix**
   - Affine transformation
   - Preserve star-convexity

2. **Elastic Deformation**
   - Non-rigid transformation
   - MÃ´ phá»ng biological variation

3. **Flipping**
   - Mirror symmetry augmentation

#### 9.1.3. Morphological Operations

1. **Distance Transform** (trong radial distance prediction)
   - TÃ­nh khoáº£ng cÃ¡ch tá»« tÃ¢m Ä‘áº¿n biÃªn
   - Biá»ƒu diá»…n compact cá»§a shapes

2. **Connected Components** (trong post-processing)
   - GÃ¡n labels cho instances
   - Watershed-like separation

#### 9.1.4. Deep Learning-based

1. **Convolutional Neural Networks**
   - Feature extraction: edge, texture, shape
   - Multi-scale analysis: U-Net hierarchy

2. **Semantic Segmentation**
   - Pixel-wise classification
   - Encoder-decoder architecture

3. **Instance Segmentation**
   - Object detection + segmentation
   - Star-convex polygon representation

### 9.2. ÄÃ³ng gÃ³p cá»§a Project

1. **PhÆ°Æ¡ng phÃ¡p má»›i**: 
   - Káº¿t há»£p pre-labeling Ä‘á»ƒ tÄƒng tá»‘c annotation
   - Chiáº¿n lÆ°á»£c chá»n frames thÃ´ng minh (temporal + brightness diversity)

2. **Cáº£i tiáº¿n hiá»‡u nÄƒng**:
   - Tá»« AP 0.72 â†’ 0.79 (+7% relative improvement)
   - Strong augmentation pipeline

3. **Practical deployment**:
   - Scripts tá»± Ä‘á»™ng cho toÃ n bá»™ workflow
   - Detailed documentation vÃ  analysis

### 9.3. BÃ i há»c Kinh nghiá»‡m

1. **Data quality > Quantity**
   - 150 áº£nh cháº¥t lÆ°á»£ng cao > 500 áº£nh bá»«a
   - Annotation cáº©n tháº­n quan trá»ng nháº¥t

2. **Augmentation is crucial**
   - TÄƒng 10-15% AP chá»‰ báº±ng augmentation
   - Äáº·c biá»‡t quan trá»ng vá»›i small dataset

3. **Proper normalization matters**
   - Percentile normalization >> min-max
   - Per-channel normalization giÃºp nhiá»u

4. **Hyperparameter tuning**
   - Thresholds (prob, nms) cáº§n fine-tune theo dataset
   - Learning rate schedule quan trá»ng

### 9.4. HÆ°á»›ng PhÃ¡t triá»ƒn

**Ngáº¯n háº¡n:**
1. TÄƒng dataset lÃªn 300-500 frames â†’ AP > 0.85
2. Thá»­ n_rays = 96 hoáº·c 128 (chi tiáº¿t hÆ¡n)
3. Ensemble nhiá»u models (TTA - Test Time Augmentation)

**DÃ i háº¡n:**
1. Multi-class segmentation (phÃ¢n loáº¡i types of cells)
2. Tracking cells qua time (video analysis)
3. 3D segmentation (z-stack images)
4. Real-time processing (optimize inference speed)

### 9.5. á»¨ng dá»¥ng Thá»±c táº¿

**NghiÃªn cá»©u sinh há»c:**
- Äáº¿m táº¿ bÃ o tá»± Ä‘á»™ng
- PhÃ¢n tÃ­ch hÃ¬nh thÃ¡i (morphology analysis)
- NghiÃªn cá»©u Ä‘á»™ng há»c táº¿ bÃ o (cell dynamics)

**Y há»c:**
- Cháº©n Ä‘oÃ¡n bá»‡nh tá»« áº£nh blood smear
- PhÃ¢n tÃ­ch mÃ´ bá»‡nh há»c (histopathology)
- Drug screening (test thuá»‘c)

**CÃ´ng nghiá»‡p:**
- Quality control trong sáº£n xuáº¥t
- Automated microscopy systems
- High-throughput screening

---

## PHá»¤ Lá»¤C

### A. CÃ´ng thá»©c ToÃ¡n há»c Tá»•ng há»£p

**1. IoU (Intersection over Union)**
$$
\text{IoU}(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$

**2. Dice Coefficient**
$$
\text{Dice}(A, B) = \frac{2|A \cap B|}{|A| + |B|}
$$

**3. Loss Function**
$$
\mathcal{L} = \lambda_{prob} \mathcal{L}_{BCE}(p, \hat{p}) + \lambda_{dist} \mathcal{L}_{MAE}(d, \hat{d})
$$

**4. Average Precision**
$$
AP = \int_0^1 P(R) \, dR
$$

**5. Precision & Recall**
$$
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
$$

**6. F1-Score**
$$
F1 = 2 \cdot \frac{P \cdot R}{P + R} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
$$

### B. Tham sá»‘ MÃ´ hÃ¬nh Chi tiáº¿t

```json
{
  "model_name": "stardist_my_data_v2_improved",
  "architecture": {
    "backbone": "U-Net",
    "n_depth": 3,
    "n_filter_base": 32,
    "n_conv_per_depth": 2,
    "kernel_size": [3, 3],
    "pool_size": [2, 2],
    "activation": "relu",
    "dropout": 0.0,
    "batch_norm": false
  },
  "stardist_config": {
    "n_rays": 64,
    "grid": [2, 2],
    "n_classes": null,
    "net_conv_after_unet": 128
  },
  "training": {
    "patch_size": [256, 256],
    "batch_size": 4,
    "epochs": 150,
    "steps_per_epoch": 200,
    "learning_rate": 0.0003,
    "optimizer": "Adam",
    "loss_weights": [1.0, 0.2],
    "foreground_ratio": 0.9,
    "background_reg": 0.0001
  },
  "augmentation": {
    "rotation": true,
    "flip": true,
    "elastic": true,
    "brightness": "Â±30%",
    "contrast": "Â±20%",
    "noise": "Ïƒ=0.01"
  },
  "thresholds": {
    "prob": 0.5,
    "nms": 0.4
  },
  "performance": {
    "AP@0.5": 0.79,
    "AP@0.7": 0.83,
    "Precision@0.5": 0.82,
    "Recall@0.5": 0.76,
    "F1@0.5": 0.79
  }
}
```

### C. Requirements

```txt
tensorflow>=2.11.0
stardist>=0.8.3
csbdeep>=0.7.2
numpy<2.0.0
opencv-python-headless<4.10
scikit-image>=0.19.0
matplotlib>=3.5.0
pandas>=1.4.0
tqdm>=4.64.0
pillow>=9.0.0
```

### D. TÃ i liá»‡u Tham kháº£o

1. **StarDist Paper**: Schmidt et al. (2018) "Cell Detection with Star-convex Polygons"
2. **U-Net Paper**: Ronneberger et al. (2015) "U-Net: Convolutional Networks for Biomedical Image Segmentation"
3. **Data Augmentation**: Shorten & Khoshgoftaar (2019) "A survey on Image Data Augmentation"
4. **Instance Segmentation Survey**: Hafiz & Bhat (2020) "A survey on instance segmentation"

---

**BÃO CÃO ÄÆ¯á»¢C HOÃ€N THÃ€NH Bá»I: GitHub Copilot**  
**NGÃ€Y: 27/11/2025**  
**Dá»° ÃN: PhÃ¡t hiá»‡n vÃ  PhÃ¢n Ä‘oáº¡n Táº¿ bÃ o sá»­ dá»¥ng StarDist**
